[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Zero-2-minimal-implementation/index.html",
    "href": "posts/Zero-2-minimal-implementation/index.html",
    "title": "Zero-2",
    "section": "",
    "text": "Scaling Optimizers with a Minimal ZeRO-2 Implementation in PyTorch In modern large-scale deep learning, the optimizer (and not just the model) becomes a bottleneck. As parameter counts scale into the billions, naively storing optimizer state (e.g.Â Adamâ€™s first and second moment estimates) on every GPU quickly exceeds memory budgets. Microsoftâ€™s ZeRO (Zero Redundancy Optimizer) [Rajbhandari et al., 2020] introduced a family of techniques for sharding optimizer state, gradients, and parameters across distributed workers. This drastically reduces memory overhead while preserving synchronous training semantics. In this post, Iâ€™ll walk through a minimal ZeRO-2 implementation in PyTorch: a custom optimizer that shards gradients and optimizer states across workers, but Iâ€™ll be keeping parameters replicated. Iâ€™ll show how it integrates with PyTorchâ€™s distributed primitives and how I benchmarked its memory and communication efficiency.\nWhy ZeRO-2? At a high level, ZeRO splits training across GPUs in three progressive stages: Stage 1 (Optimizer State Sharding): Each worker only holds the optimizer states for a shard of parameters. Stage 2 (Gradient Sharding): Gradients are also sharded across workers, reducing memory overhead further. Stage 3 (Parameter Sharding): Parameters themselves are partitioned, removing replication altogether. My code implements Stage 2. Parameters remain replicated (so forward/backward is standard), but gradients and optimizer states are sharded. This yields big memory savings while retaining straightforward data-parallel semantics.\nA Minimal ZeRO-2 Optimizer in PyTorch The core of the implementation is the Zero2Adam class. It mirrors PyTorchâ€™s torch.optim.Adam but uses distributed primitives (reduce_scatter and broadcast) to shard work across GPUs.\nStep 1: Flatten parameters and grads Instead of managing parameter tensors individually, I flatten all parameters into a single 1D buffer. This makes sharding and communication simpler:\nflat_param = torch.cat([p.data.view(-1) for p in params], dim=0)\nflat_grad  = torch.cat([p.grad.view(-1) for p in params], dim=0)\nNow, optimizer state can be managed as slices into this flat buffer.\nStep 2: Reduce-Scatter gradients Each rank computes local gradients, then participates in a reduce_scatter:\ndist.reduce_scatter(recv, in_chunks, op=dist.ReduceOp.SUM)\nThis operation both sums gradients across ranks and scatters shards of the summed result. After dividing by world_size, each rank holds only its local shard of the global gradient. This eliminates full replication of gradient buffers.\nStep 3: Adam update on local shard Each rank maintains optimizer states (m, v) only for its shard. Updates are applied locally:\nself._m.mul_(b1).add_(g, alpha=1-b1)\nself._v.mul_(b2).addcmul_(g, g, value=1-b2)\n\nm_hat = self._m / (1 - b1**t)\nv_hat = self._v / (1 - b2**t)\nlocal_param.addcdiv_(m_hat, v_hat.sqrt().add(eps), value=-lr)\nThis keeps memory proportional to 1 / world_size.\nStep 4: Broadcast updated shards After updating, parameters must be synchronized across ranks. Each rank broadcasts its updated shard:\ndist.broadcast(seg, src=src)\nAt the end of this step, all workers have identical model weights, ready for the next forward pass.\nStep 5: Scatter back to original tensors Finally, the flat buffer is scattered back into the original parameter tensors:\nfor p in self.params:\n    p.data.copy_(flat_param[offset:offset+n].view_as(p.data))\nThis maintains PyTorch compatibility with minimal changes.\nCommunication & Memory Efficiency\nA key aspect of this implementation of ZeRO-2 is reducing redundant memory: With vanilla Adam, each rank stores: Parameters: O(N) Gradients: O(N) Adam states (m, v): O(2N) Total: 4N per rank\nWith ZeRO-2: Parameters: O(N) (still replicated) Gradients: O(N / world_size) Adam states: O(2N / world_size) Total: ~N + 3N / world_size per rank For large world sizes, this approaches O(N) per rank instead of O(4N).\nTraining loop and benchmarking:\nThe training loop uses synthetic data (10k-dim vectors) to stress memory and communication:\nfor epoch in range(epochs):\n    loss = F.mse_loss(model(x), y)\n    loss.backward()\n    opt.step()\nAfter each epoch, I log peak GPU memory and communication statistics: example: [Epoch 0] loss=0.9981 peak_mem=455.12MB [Stats] reduce_scatter / step: 1.0 broadcast / step: 8.0 avg step time: 0.034s avg comm time: 0.011s\nWhy This Matters This vanilla implementation of ZeRO-2 optimizer shows how distributed primitives can drastically reduce memory overhead without rewriting the training loop. Itâ€™s only a few hundred lines of code, but captures the essence of a technique that underpins training of models like GPT-3 and beyond. What makes this elegant is that the forward/backward path is untouchedâ€”only the optimizer is modified. This is why ZeRO became such a practical and impactful idea: it slots into existing training pipelines with minimal disruption, but unlocks model scales that would otherwise be infeasible.\nBelow is the full working code that works using modal, which i separated into 2 files - one for the implementation and one for the code execution\n# modal_zero2.py\nimport os\nimport subprocess\n\nimport modal\nfrom modal import Image\n\nimage = (\n    Image.from_registry(\"pytorch/pytorch:2.3.1-cuda12.1-cudnn8-runtime\")\n    .env({\n        \"HF_HOME\": \"/workspace/.cache/huggingface\",\n        \"NCCL_DEBUG\": \"WARN\",\n        \"NCCL_IB_DISABLE\": \"1\",\n        \"TORCH_NCCL_BLOCKING_WAIT\": \"0\",\n    })\n    # ðŸ‘‡ LAST: include your training script\n    .add_local_file(\"zero2_train.py\", remote_path=\"/workspace/zero2_train.py\")\n)\n\napp = modal.App(\"zero2-modal\")\n\n@app.function(image=image, gpu=\"A100-40GB:8\", timeout=24*60*60)\ndef run_zero2(nproc_per_node: int = 8, epochs: int = 100):\n    os.chdir(\"/workspace\")\n    torchrun_cmd = [\"torchrun\", f\"--nproc_per_node={nproc_per_node}\", \"zero2_train.py\", f\"--epochs={epochs}\"]\n    fallback_cmd = [\"python\", \"-m\", \"torch.distributed.run\", f\"--nproc_per_node={nproc_per_node}\", \"zero2_train.py\", f\"--epochs={epochs}\"]\n\n    def run(cmd):\n        print(\"Launching:\", \" \".join(cmd), flush=True)\n        p = subprocess.Popen(cmd); p.wait(); return p.returncode\n\n    rc = run(torchrun_cmd)\n    if rc != 0:\n        print(\"torchrun failed; retrying with python -m torch.distributed.run\", flush=True)\n        rc = run(fallback_cmd)\n    if rc != 0:\n        raise SystemExit(rc)\n    print(\"Training completed âœ”\", flush=True)\n\n@app.local_entrypoint()\ndef main(nproc: int = 8, epochs: int = 100):\n    run_zero2.remote(nproc, epochs)\n# zero2_train.py\nimport argparse\nimport time\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nfrom torch.optim import (\n    Adam,\n    Optimizer,\n)\n\n\ndef set_seed(seed: int):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nclass Zero2Adam:\n    \"\"\"\n    Minimal ZeRO-2 optimizer:\n      - Params replicated across ranks\n      - Gradients + optimizer state are sharded across ranks (flat buffer)\n      - Each rank updates only its local shard, then all shards are broadcast\n    \"\"\"\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.0):\n        self.params = [p for p in params if p.requires_grad]\n\n        # Dist\n        self.world_size = dist.get_world_size()\n        self.rank = dist.get_rank()\n\n        # Adam hyperparams\n        self.lr = lr\n        self.betas = betas\n        self.eps = eps\n        self.weight_decay = weight_decay\n\n        # Lazily built\n        self._flat_shapes = None\n        self._flat_offsets = None\n        self._numel_total = None\n        self._shard_start = None\n        self._shard_end = None\n        self._shard_len = None\n\n        # Adam state for local shard\n        self._m = None\n        self._v = None\n        self._t = 0\n        self._state_init = False\n\n        # Stats\n        self.communication_time = 0.0\n        self.step_time = 0.0\n        self.reduce_scatter_count = 0\n        self.broadcast_count = 0\n\n    # ---------- helpers ----------\n\n    def _build_flat_views(self):\n        with torch.no_grad():\n            device = self.params[0].device\n            dtype = self.params[0].dtype\n            shapes, offsets, chunks = [], [], []\n            off = 0\n            for p in self.params:\n                n = p.numel()\n                shapes.append(p.data.shape)\n                offsets.append((off, off + n))\n                off += n\n                chunks.append(p.data.view(-1))\n            flat_param = torch.cat(chunks, dim=0).to(device=device, dtype=dtype)\n            self._flat_shapes = shapes\n            self._flat_offsets = offsets\n            self._numel_total = flat_param.numel()\n            return flat_param\n\n    def _flat_grads(self):\n        device = self.params[0].device\n        grads = []\n        for p in self.params:\n            if p.grad is None:\n                grads.append(torch.zeros_like(p.data, device=device))\n            else:\n                grads.append(p.grad.data)\n        return torch.cat([g.view(-1) for g in grads], dim=0)\n\n    def _compute_shard_bounds(self, total_numel: int):\n        shard = (total_numel + self.world_size - 1) // self.world_size\n        start = self.rank * shard\n        end = min(start + shard, total_numel)\n        return start, end, shard\n\n    def _init_state_if_needed(self, flat_param):\n        if self._state_init:\n            return\n        total = flat_param.numel()\n        start, end, shard = self._compute_shard_bounds(total)\n        self._shard_start, self._shard_end = start, end\n        self._shard_len = end - start\n\n        device = flat_param.device\n        dtype = flat_param.dtype\n        self._m = torch.zeros(self._shard_len, device=device, dtype=dtype)\n        self._v = torch.zeros(self._shard_len, device=device, dtype=dtype)\n        self._t = 0\n        self._state_init = True\n\n    def zero_grad(self):\n        for p in self.params:\n            p.grad = None\n\n    @torch.no_grad()\n    def step(self):\n        step_start = time.perf_counter()\n        device = self.params[0].device\n\n        # 1) Flat params & grads\n        flat_param = self._build_flat_views()\n        self._init_state_if_needed(flat_param)\n        flat_grad = self._flat_grads()\n\n        # 2) Reduce-scatter to local grad shard\n        comm_start = time.perf_counter()\n        pad = (self.world_size - (flat_grad.numel() % self.world_size)) % self.world_size\n        if pad:\n            flat_grad = torch.nn.functional.pad(flat_grad, (0, pad))\n        chunk_sz = flat_grad.numel() // self.world_size\n        in_chunks = list(flat_grad.split(chunk_sz, dim=0))\n        recv = torch.empty(chunk_sz, device=device, dtype=flat_grad.dtype)\n        dist.reduce_scatter(recv, in_chunks, op=dist.ReduceOp.SUM)\n        self.reduce_scatter_count += 1\n\n        local_len = self._shard_end - self._shard_start\n        local_grad = recv[:local_len].div_(self.world_size)\n\n        torch.cuda.synchronize()\n        self.communication_time += time.perf_counter() - comm_start\n\n        # 3) Adam update on local shard\n        self._t += 1\n        b1, b2 = self.betas\n        local_param = flat_param[self._shard_start:self._shard_end]\n        g = local_grad\n        if self.weight_decay:\n            g = g.add(local_param, alpha=self.weight_decay)\n\n        self._m.mul_(b1).add_(g, alpha=1 - b1)\n        self._v.mul_(b2).addcmul_(g, g, value=1 - b2)\n        m_hat = self._m / (1 - b1 ** self._t)\n        v_hat = self._v / (1 - b2 ** self._t)\n        local_param.addcdiv_(m_hat, v_hat.sqrt().add(self.eps), value=-self.lr)\n        flat_param[self._shard_start:self._shard_end] = local_param\n\n        # 4) Broadcast each shard from its owner so all ranks sync full params\n        total = self._numel_total\n        _, _, shard_span = self._compute_shard_bounds(total)\n        for src in range(self.world_size):\n            s = src * shard_span\n            e = min(s + shard_span, total)\n            if dist.get_rank() == src:\n                seg = flat_param[s:e]\n            else:\n                seg = torch.empty(e - s, device=device, dtype=flat_param.dtype)\n            dist.broadcast(seg, src=src)\n            self.broadcast_count += 1\n            if dist.get_rank() != src:\n                flat_param[s:e] = seg\n\n        # 5) Scatter flat back to params; clear grads\n        off = 0\n        for p in self.params:\n            n = p.numel()\n            p.data.copy_(flat_param[off:off + n].view_as(p.data))\n            p.grad = None\n            off += n\n\n        torch.cuda.synchronize()\n        self.step_time += time.perf_counter() - step_start\n\n\ndef build_model(d: int = 10_000, depth: int = 6):\n    layers = []\n    for _ in range(depth):\n        layers += [nn.Linear(d, d), nn.ReLU()]\n    layers += [nn.Linear(d, d)]\n    return nn.Sequential(*layers)\n\n\ndef train_loop(epochs: int, device: torch.device):\n    rank = dist.get_rank()\n    world_size = dist.get_world_size()\n    if rank == 0:\n        print(f\"[Init] world_size={world_size}\")\n\n    model = build_model().to(device)\n    base_opt = Adam(model.parameters(), lr=1e-3)  # only for param listing\n    opt = Zero2Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.0)\n\n    # synthetic data\n    bsz = 16\n    x = torch.randn(bsz, 10_000, device=device)\n    y = torch.randn(bsz, 10_000, device=device)\n\n    # warmup\n    opt.zero_grad()\n    loss = nn.functional.mse_loss(model(x), y)\n    loss.backward()\n    opt.step()\n    torch.cuda.synchronize()\n    # reset stats\n    opt.communication_time = 0.0\n    opt.step_time = 0.0\n    opt.reduce_scatter_count = 0\n    opt.broadcast_count = 0\n\n    for epoch in range(epochs):\n        torch.cuda.reset_peak_memory_stats(device)\n        opt.zero_grad()\n        out = model(x)\n        loss = nn.functional.mse_loss(out, y)\n        loss.backward()\n        opt.step()\n\n        peak_mb = torch.cuda.max_memory_allocated(device) / 1024**2\n        if rank == 0:\n            print(f\"[Epoch {epoch}] loss={loss.item():.4f} peak_mem={peak_mb:.2f}MB\")\n\n    if rank == 0:\n        avg_steps = max(1, epochs)\n        print(\"\\n[Stats]\")\n        print(f\"reduce_scatter / step: {opt.reduce_scatter_count/avg_steps:.1f}\")\n        print(f\"broadcast      / step: {opt.broadcast_count/avg_steps:.1f}\")\n        print(f\"avg step time:          {opt.step_time/avg_steps:.3f}s\")\n        print(f\"avg comm time:          {opt.communication_time/avg_steps:.3f}s\")\n        print(f\"avg compute time:       {(opt.step_time - opt.communication_time)/avg_steps:.3f}s\")\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--epochs\", type=int, default=3)\n    parser.add_argument(\"--seed\", type=int, default=42)\n    args = parser.parse_args()\n\n    # torchrun provides LOCAL_RANK / RANK / WORLD_SIZE\n    local_rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n    torch.cuda.set_device(local_rank)\n\n    set_seed(args.seed)\n    dist.init_process_group(\"nccl\")\n\n    device = torch.device(f\"cuda:{local_rank}\")\n    train_loop(args.epochs, device)\n\n    dist.destroy_process_group()\n\n\nif __name__ == \"__main__\":\n    import os\n    main()\nReults of 2 runs:\nLaunching: torchrun â€“nproc_per_node=4 zero2_train.py â€“epochs=100 [Init] world_size=4 NCCL version 2.20.5+cuda12.1 [Epoch 0] loss=21496.0859 peak_mem=15382.90MB [Epoch 1] loss=1.0013 peak_mem=15382.90MB [Epoch 2] loss=0.9970 peak_mem=15382.90MB [Epoch 3] loss=0.9674 peak_mem=15382.90MB [Epoch 4] loss=2.4202 peak_mem=15382.90MB [Epoch 5] loss=0.9618 peak_mem=15382.90MB [Epoch 6] loss=0.9901 peak_mem=15382.90MB [Epoch 7] loss=0.9950 peak_mem=15382.90MB [Epoch 8] loss=0.9904 peak_mem=15382.90MB [Epoch 9] loss=0.9757 peak_mem=15382.90MB [Epoch 10] loss=0.9704 peak_mem=15382.90MB [Epoch 11] loss=0.9503 peak_mem=15382.90MB [Epoch 12] loss=0.9457 peak_mem=15382.90MB [Epoch 13] loss=0.9525 peak_mem=15382.90MB [Epoch 14] loss=0.9526 peak_mem=15382.90MB [Epoch 15] loss=0.9511 peak_mem=15382.90MB [Epoch 16] loss=0.9418 peak_mem=15382.90MB [Epoch 17] loss=0.9518 peak_mem=15382.90MB [Epoch 18] loss=0.9413 peak_mem=15382.90MB [Epoch 19] loss=0.9471 peak_mem=15382.90MB [Epoch 20] loss=0.9427 peak_mem=15382.90MB [Epoch 21] loss=0.9376 peak_mem=15382.90MB [Epoch 22] loss=0.9426 peak_mem=15382.90MB [Epoch 23] loss=0.9346 peak_mem=15382.90MB [Epoch 24] loss=0.9335 peak_mem=15382.90MB [Epoch 25] loss=0.9314 peak_mem=15382.90MB [Epoch 26] loss=0.9230 peak_mem=15382.90MB [Epoch 27] loss=0.9154 peak_mem=15382.90MB [Epoch 28] loss=0.9099 peak_mem=15382.90MB [Epoch 29] loss=0.8996 peak_mem=15382.90MB [Epoch 30] loss=0.8942 peak_mem=15382.90MB [Epoch 31] loss=0.8908 peak_mem=15382.90MB [Epoch 32] loss=0.8951 peak_mem=15382.90MB [Epoch 33] loss=0.8831 peak_mem=15382.90MB [Epoch 34] loss=0.8814 peak_mem=15382.90MB [Epoch 35] loss=0.8836 peak_mem=15382.90MB [Epoch 36] loss=0.8794 peak_mem=15382.90MB [Epoch 37] loss=0.8783 peak_mem=15382.90MB [Epoch 38] loss=0.8794 peak_mem=15382.90MB [Epoch 39] loss=0.8775 peak_mem=15382.90MB [Epoch 40] loss=0.8778 peak_mem=15382.90MB [Epoch 41] loss=0.8765 peak_mem=15382.90MB [Epoch 42] loss=0.8759 peak_mem=15382.90MB [Epoch 43] loss=0.8749 peak_mem=15382.90MB [Epoch 44] loss=0.8727 peak_mem=15382.90MB [Epoch 45] loss=0.8703 peak_mem=15382.90MB [Epoch 46] loss=0.8662 peak_mem=15382.90MB [Epoch 47] loss=0.8593 peak_mem=15382.90MB [Epoch 48] loss=0.8469 peak_mem=15382.90MB [Epoch 49] loss=0.8331 peak_mem=15382.90MB [Epoch 50] loss=0.8183 peak_mem=15382.90MB [Epoch 51] loss=0.8073 peak_mem=15382.90MB [Epoch 52] loss=0.8059 peak_mem=15382.90MB [Epoch 53] loss=0.7996 peak_mem=15382.90MB [Epoch 54] loss=0.7966 peak_mem=15382.90MB [Epoch 55] loss=0.7875 peak_mem=15382.90MB [Epoch 56] loss=0.7802 peak_mem=15382.90MB [Epoch 57] loss=0.7760 peak_mem=15382.90MB [Epoch 58] loss=0.7737 peak_mem=15382.90MB [Epoch 59] loss=0.7627 peak_mem=15382.90MB [Epoch 60] loss=0.7592 peak_mem=15382.90MB [Epoch 61] loss=0.7539 peak_mem=15382.90MB [Epoch 62] loss=0.7466 peak_mem=15382.90MB [Epoch 63] loss=0.7384 peak_mem=15382.90MB [Epoch 64] loss=0.7301 peak_mem=15382.90MB [Epoch 65] loss=0.7226 peak_mem=15382.90MB [Epoch 66] loss=0.7140 peak_mem=15382.90MB [Epoch 67] loss=0.7052 peak_mem=15382.90MB [Epoch 68] loss=0.6921 peak_mem=15382.90MB [Epoch 69] loss=0.6763 peak_mem=15382.90MB [Epoch 70] loss=0.6630 peak_mem=15382.90MB [Epoch 71] loss=0.6531 peak_mem=15382.90MB [Epoch 72] loss=0.6527 peak_mem=15382.90MB [Epoch 73] loss=0.6425 peak_mem=15382.90MB [Epoch 74] loss=0.6357 peak_mem=15382.90MB [Epoch 75] loss=0.6330 peak_mem=15382.90MB [Epoch 76] loss=0.6259 peak_mem=15382.90MB [Epoch 77] loss=0.6224 peak_mem=15382.90MB [Epoch 78] loss=0.6163 peak_mem=15382.90MB [Epoch 79] loss=0.6109 peak_mem=15382.90MB [Epoch 80] loss=0.6087 peak_mem=15382.90MB [Epoch 81] loss=0.5960 peak_mem=15382.90MB [Epoch 82] loss=0.5903 peak_mem=15382.90MB [Epoch 83] loss=0.5762 peak_mem=15382.90MB [Epoch 84] loss=0.5665 peak_mem=15382.90MB [Epoch 85] loss=0.5545 peak_mem=15382.90MB [Epoch 86] loss=0.5452 peak_mem=15382.90MB [Epoch 87] loss=0.5360 peak_mem=15382.90MB [Epoch 88] loss=0.5253 peak_mem=15382.90MB [Epoch 89] loss=0.5173 peak_mem=15382.90MB [Epoch 90] loss=0.5073 peak_mem=15382.90MB [Epoch 91] loss=0.4968 peak_mem=15382.90MB [Epoch 92] loss=0.4885 peak_mem=15382.90MB [Epoch 93] loss=0.4920 peak_mem=15382.90MB [Epoch 94] loss=0.4731 peak_mem=15382.90MB [Epoch 95] loss=0.4484 peak_mem=15382.90MB [Epoch 96] loss=0.4351 peak_mem=15382.90MB [Epoch 97] loss=0.4217 peak_mem=15382.90MB [Epoch 98] loss=0.4077 peak_mem=15382.90MB [Epoch 99] loss=0.3981 peak_mem=15382.90MB [Stats] reduce_scatter / step: 1.0 broadcast / step: 4.0 avg step time: 0.064s avg comm time: 0.030s avg compute time: 0.034s Training completed âœ” Launching: torchrun â€“nproc_per_node=8 zero2_train.py â€“epochs=100 [Init] world_size=8 NCCL version 2.20.5+cuda12.1 [Epoch 0] loss=21496.0859 peak_mem=14380.90MB [Epoch 1] loss=1.0013 peak_mem=14380.90MB [Epoch 2] loss=0.9970 peak_mem=14380.90MB [Epoch 3] loss=0.9674 peak_mem=14380.90MB [Epoch 4] loss=2.4202 peak_mem=14380.90MB [Epoch 5] loss=0.9618 peak_mem=14380.90MB [Epoch 6] loss=0.9901 peak_mem=14380.90MB [Epoch 7] loss=0.9951 peak_mem=14380.90MB [Epoch 8] loss=0.9904 peak_mem=14380.90MB [Epoch 9] loss=0.9757 peak_mem=14380.90MB [Epoch 10] loss=0.9704 peak_mem=14380.90MB [Epoch 11] loss=0.9504 peak_mem=14380.90MB [Epoch 12] loss=0.9460 peak_mem=14380.90MB [Epoch 13] loss=0.9526 peak_mem=14380.90MB [Epoch 14] loss=0.9512 peak_mem=14380.90MB [Epoch 15] loss=0.9494 peak_mem=14380.90MB [Epoch 16] loss=0.9421 peak_mem=14380.90MB [Epoch 17] loss=0.9484 peak_mem=14380.90MB [Epoch 18] loss=0.9416 peak_mem=14380.90MB [Epoch 19] loss=0.9457 peak_mem=14380.90MB [Epoch 20] loss=0.9405 peak_mem=14380.90MB [Epoch 21] loss=0.9393 peak_mem=14380.90MB [Epoch 22] loss=0.9399 peak_mem=14380.90MB [Epoch 23] loss=0.9340 peak_mem=14380.90MB [Epoch 24] loss=0.9343 peak_mem=14380.90MB [Epoch 25] loss=0.9303 peak_mem=14380.90MB [Epoch 26] loss=0.9233 peak_mem=14380.90MB [Epoch 27] loss=0.9179 peak_mem=14380.90MB [Epoch 28] loss=0.9084 peak_mem=14380.90MB [Epoch 29] loss=0.9034 peak_mem=14380.90MB [Epoch 30] loss=0.8992 peak_mem=14380.90MB [Epoch 31] loss=0.8877 peak_mem=14380.90MB [Epoch 32] loss=0.8912 peak_mem=14380.90MB [Epoch 33] loss=0.8802 peak_mem=14380.90MB [Epoch 34] loss=0.8824 peak_mem=14380.90MB [Epoch 35] loss=0.8806 peak_mem=14380.90MB [Epoch 36] loss=0.8812 peak_mem=14380.90MB [Epoch 37] loss=0.8801 peak_mem=14380.90MB [Epoch 38] loss=0.8785 peak_mem=14380.90MB [Epoch 39] loss=0.8774 peak_mem=14380.90MB [Epoch 40] loss=0.8788 peak_mem=14380.90MB [Epoch 41] loss=0.8771 peak_mem=14380.90MB [Epoch 42] loss=0.8743 peak_mem=14380.90MB [Epoch 43] loss=0.8734 peak_mem=14380.90MB [Epoch 44] loss=0.8710 peak_mem=14380.90MB [Epoch 45] loss=0.8683 peak_mem=14380.90MB [Epoch 46] loss=0.8633 peak_mem=14380.90MB [Epoch 47] loss=0.8566 peak_mem=14380.90MB [Epoch 48] loss=0.8529 peak_mem=14380.90MB [Epoch 49] loss=0.8339 peak_mem=14380.90MB [Epoch 50] loss=0.8157 peak_mem=14380.90MB [Epoch 51] loss=0.7987 peak_mem=14380.90MB [Epoch 52] loss=0.7999 peak_mem=14380.90MB [Epoch 53] loss=0.7692 peak_mem=14380.90MB [Epoch 54] loss=0.7672 peak_mem=14380.90MB [Epoch 55] loss=0.7537 peak_mem=14380.90MB [Epoch 56] loss=0.7445 peak_mem=14380.90MB [Epoch 57] loss=0.7436 peak_mem=14380.90MB [Epoch 58] loss=0.7358 peak_mem=14380.90MB [Epoch 59] loss=0.7283 peak_mem=14380.90MB [Epoch 60] loss=0.7241 peak_mem=14380.90MB [Epoch 61] loss=0.7181 peak_mem=14380.90MB [Epoch 62] loss=0.7107 peak_mem=14380.90MB [Epoch 63] loss=0.7072 peak_mem=14380.90MB [Epoch 64] loss=0.6992 peak_mem=14380.90MB [Epoch 65] loss=0.6903 peak_mem=14380.90MB [Epoch 66] loss=0.6811 peak_mem=14380.90MB [Epoch 67] loss=0.6731 peak_mem=14380.90MB [Epoch 68] loss=0.6628 peak_mem=14380.90MB [Epoch 69] loss=0.6492 peak_mem=14380.90MB [Epoch 70] loss=0.6375 peak_mem=14380.90MB [Epoch 71] loss=0.6247 peak_mem=14380.90MB [Epoch 72] loss=0.6078 peak_mem=14380.90MB [Epoch 73] loss=0.5926 peak_mem=14380.90MB [Epoch 74] loss=0.5793 peak_mem=14380.90MB [Epoch 75] loss=0.5666 peak_mem=14380.90MB [Epoch 76] loss=0.5571 peak_mem=14380.90MB [Epoch 77] loss=0.5529 peak_mem=14380.90MB [Epoch 78] loss=0.5428 peak_mem=14380.90MB [Epoch 79] loss=0.5325 peak_mem=14380.90MB [Epoch 80] loss=0.5249 peak_mem=14380.90MB [Epoch 81] loss=0.5177 peak_mem=14380.90MB [Epoch 82] loss=0.5086 peak_mem=14380.90MB [Epoch 83] loss=0.4968 peak_mem=14380.90MB [Epoch 84] loss=0.4864 peak_mem=14380.90MB [Epoch 85] loss=0.4787 peak_mem=14380.90MB [Epoch 86] loss=0.4646 peak_mem=14380.90MB [Epoch 87] loss=0.4540 peak_mem=14380.90MB [Epoch 88] loss=0.4443 peak_mem=14380.90MB [Epoch 89] loss=0.4353 peak_mem=14380.90MB [Epoch 90] loss=0.4323 peak_mem=14380.90MB [Epoch 91] loss=0.4189 peak_mem=14380.90MB [Epoch 92] loss=0.3985 peak_mem=14380.90MB [Epoch 93] loss=0.3899 peak_mem=14380.90MB [Epoch 94] loss=0.3807 peak_mem=14380.90MB [Epoch 95] loss=0.3647 peak_mem=14380.90MB [Epoch 96] loss=0.3544 peak_mem=14380.90MB [Epoch 97] loss=0.3462 peak_mem=14380.90MB [Epoch 98] loss=0.3372 peak_mem=14380.90MB [Epoch 99] loss=0.3282 peak_mem=14380.90MB [Stats] reduce_scatter / step: 1.0 broadcast / step: 8.0 avg step time: 0.061s avg comm time: 0.032s avg compute time: 0.029s Training completed âœ”\nResults:\nSharding Optimizer State & Gradients, Measured\nI ran the minimal ZeRO-2 optimizer on 4Ã— and 8Ã— A100s (NCCL 2.20.5 + CUDA 12.1) and logged loss, peak memory, and per-step timing and comm stats.\nCommunication pattern per training step: 1Ã— reduce_scatter (sum + scatter the flat grad) world_size broadcasts (each rank owns a param shard and broadcasts it)\n\n\nHeadline numbers Peak memory per GPU drops from 15.38 GB â†’ 14.04 GB (â‰ˆ 0.98 GB reduction) moving 4 â†’ 8 GPUs. Avg step time improves from 0.064 s â†’ 0.061 s (~ 1.05Ã— speedup). Broadcasts/step = world_size (4 vs 8), reduce_scatter/step = 1 (by design). Compute time shrinks (0.034 s â†’ 0.029 s) as each rank updates a smaller shard; comm time edges up (0.030 s â†’ 0.032 s) due to more shards to synchronize.\nWhy memory falls only ~1 GB? ZeRO-2 saves memory on gradients + optimizer state but keeps parameters replicated. Using the idealized model and fitting the 4â†’8 GPU drop (~0.98 GB) implies ~2.6â€“2.7 GB of parameters and a large constant C (activations, CUDA context, transient grads during backward, fragmentation, etc.). In my minimalist trainer, full per-parameter p.grad tensors still materialize during backward (before flattening and reduce-scatter), which limits peak-mem gains."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Zero-2\n\n\n\npython\n\n\n\n\n\n\n\n\n\nOct 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 1, 2025\n\n\nTristan Oâ€™Malley\n\n\n\n\n\nNo matching items"
  }
]